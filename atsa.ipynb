{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prime-rouge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/revanth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/revanth/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/revanth/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "demanding-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(text):\n",
    "    \n",
    "    text = word_tokenize(text) # tokenize words in text\n",
    "    text = [re.sub('[^A-Za-z]+', '', word) for word in text] # this line substitutes any white space before the word by removing the space\n",
    "    text = [word.lower() for word in text if word.isalpha()] # lower each word in text\n",
    "    text = [WordNetLemmatizer().lemmatize(word) for word in text] # lemmatization of words, so when see persons an person, both are dealt as one word person\n",
    "    text = ' '.join(text) # join words into text again\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "earned-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    x_data = pd.read_json(path)\n",
    "    x_data['sentence'] = x_data['sentence'].apply(process_sentence)\n",
    "    y_data = pd.factorize(x_data['sentiment'])[0]\n",
    "    x_data. drop('sentiment', axis=1, inplace=True)\n",
    "    \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "regular-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = get_data('./atsa_hard_train.json')\n",
    "x_test, y_test = get_data('./atsa_hard_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "north-battle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1806"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_VOCAB = 5000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB)\n",
    "tokenizer.fit_on_texts(np.concatenate([x_train.sentence.to_numpy(), x_test.sentence.to_numpy()], axis=0))\n",
    "\n",
    "words_to_index = tokenizer.word_index\n",
    "len(words_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "automatic-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "emmbed_dict = {}\n",
    "with open('glove.6B.50d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:],'float32') \n",
    "        emmbed_dict[word]=vector\n",
    "\n",
    "maxSenLen = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "behavioral-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(words_to_index) + 1\n",
    "EMBEDDING_DIM = emmbed_dict['apple'].shape[0]\n",
    "\n",
    "emb_matrix = np.zeros((INPUT_DIM, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in words_to_index.items():\n",
    "    \n",
    "    emb_vector = emmbed_dict.get(word)\n",
    "    if emb_vector is not None:\n",
    "        emb_matrix[index] = emb_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "serious-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fa(x_data):\n",
    "    feature = tokenizer.texts_to_sequences(x_data.sentence)\n",
    "    feature = pad_sequences(feature, maxlen=maxSenLen, padding='post')\n",
    "    feature = np.array([np.array(x) for x in feature])\n",
    "\n",
    "    aspect = tokenizer.texts_to_sequences(x_data.aspect)\n",
    "    aspect = pad_sequences(aspect, maxlen=8, padding='post')\n",
    "    aspect = np.array([np.array(x) for x in aspect])\n",
    "    \n",
    "    return feature, aspect\n",
    "\n",
    "feature_train, aspect_train = fa(x_train)\n",
    "feature_test, aspect_test = fa(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "formal-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_size, n_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix, device=\"cpu\"), requires_grad=True)\n",
    "        \n",
    "        self.aspect_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.aspect_embedding.weight = nn.Parameter(torch.tensor(emb_matrix, device=\"cpu\"), requires_grad=True)\n",
    "                \n",
    "        self.convs1 = nn.Conv1d(in_channels = embedding_dim, out_channels = n_filters, kernel_size = filter_size)\n",
    "        self.convs2 = nn.Conv1d(in_channels = embedding_dim, out_channels = n_filters, kernel_size = filter_size)\n",
    "        self.convs3 = nn.Conv1d(in_channels = embedding_dim, out_channels = n_filters, kernel_size = filter_size)\n",
    "        \n",
    "        self.fc = nn.Linear(n_filters, n_classes)\n",
    "        self.aspect_fc = nn.Linear(n_filters, n_filters)\n",
    "                \n",
    "    def forward(self, text, aspect):\n",
    "                \n",
    "        embedded = self.embedding(text)\n",
    "        aspect_embedded = self.aspect_embedding(aspect)\n",
    "                        \n",
    "        embedded = embedded.permute(0, 2, 1).float()\n",
    "        aspect_embedded = aspect_embedded.permute(0, 2, 1).float()\n",
    "        \n",
    "        aspect_embedded = F.tanh(self.convs3(aspect_embedded))\n",
    "        aspect_embedded = F.max_pool1d(aspect_embedded, aspect_embedded.shape[2]).squeeze(2)\n",
    "                \n",
    "        x = F.tanh(self.convs1(embedded))\n",
    "        y = F.relu(self.convs2(embedded) + self.aspect_fc(aspect_embedded).unsqueeze(2))\n",
    "        x = x * y\n",
    "\n",
    "        pooled = F.max_pool1d(x, x.shape[2]).squeeze(2)\n",
    "            \n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fabulous-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILTERS = 10\n",
    "FILTER_SIZE = 3\n",
    "N_CLASSES = 4\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZE, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "geological-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "latest-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "engaged-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    correct = (preds.argmax(1) == y).sum()\n",
    "    acc = correct.sum() / len(y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "stylish-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, feature, aspect, optimizer):\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(feature)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ftr = torch.tensor([feature[i]])\n",
    "        ftr = ftr.to(device)\n",
    "        asp = torch.tensor([aspect[i]])\n",
    "        asp = asp.to(device)\n",
    "        \n",
    "        predictions = model(ftr, asp).squeeze(1)\n",
    "        \n",
    "        loss =  F.cross_entropy(predictions, torch.tensor(y_train[i], device=\"cpu\").view(-1))\n",
    "        \n",
    "        acc = binary_accuracy(predictions, torch.tensor(y_train[i], device=\"cpu\").view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "        \n",
    "    return total_loss / len(feature), total_acc / len(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hundred-apparatus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-19796977aada>:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  ftr = torch.tensor([feature[i]])\n",
      "/home/revanth/.local/lib/python3.8/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    train_loss, train_acc = train(model, feature_train, aspect_train, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "threatened-edmonton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9847035007138588 0.6194605009633911\n"
     ]
    }
   ],
   "source": [
    "print(train_loss, train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hundred-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, feature, aspect):\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(len(feature)):\n",
    "                \n",
    "        ftr = torch.tensor([feature[i]])\n",
    "        ftr = ftr.to(device)\n",
    "        asp = torch.tensor([aspect[i]])\n",
    "        asp = asp.to(device)\n",
    "        \n",
    "        predictions = model(ftr, asp).squeeze(1)\n",
    "        \n",
    "        loss =  F.cross_entropy(predictions, torch.tensor(y_test[i], device=\"cpu\").view(-1))\n",
    "        \n",
    "        acc = binary_accuracy(predictions, torch.tensor(y_test[i], device=\"cpu\").view(-1))\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "        \n",
    "    return total_loss / len(feature), total_acc / len(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "happy-anniversary",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = eval(model, feature_test, aspect_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "another-factor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9115045531063664 0.15510204081632653\n"
     ]
    }
   ],
   "source": [
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-accountability",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
